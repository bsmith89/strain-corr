{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "\n",
    "_os.chdir(_os.environ[\"PROJECT_ROOT\"])\n",
    "_os.path.realpath(_os.path.curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from itertools import chain, product\n",
    "from tempfile import mkstemp\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sfacts as sf\n",
    "import statsmodels.formula.api as smf\n",
    "import xarray as xr\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# from fastcluster import linkage\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "from statsmodels.graphics.regressionplots import influence_plot\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib.plot\n",
    "import lib.thisproject.data\n",
    "from lib.pandas_util import align_indexes, aligned_index, idxwhere, invert_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkage_order(linkage, labels):\n",
    "    return labels[sp.cluster.hierarchy.to_tree(linkage).pre_order(lambda x: x.id)]\n",
    "\n",
    "def label_een_experiment_sample(x):\n",
    "    if x.sample_type == \"human\":\n",
    "        label = f\"[{x.name}] {x.collection_date_relative_een_end} {x.diet_or_media}\"\n",
    "    elif x.sample_type in [\"Fermenter_inoculum\"]:\n",
    "        label = f\"[{x.name}] {x.source_samples} inoc {x.diet_or_media}\"\n",
    "    elif x.sample_type in [\"Fermenter\"]:\n",
    "        # label = f\"[{x.name}] {x.source_samples} frmnt {x.diet_or_media}\"\n",
    "        label = f\"[{x.name}] {x.source_samples} {x.diet_or_media}\"\n",
    "    elif x.sample_type in [\"mouse\"]:\n",
    "        if x.status_mouse_inflamed == 'Inflamed':\n",
    "            # label = f\"[{x.name}] {x.source_samples} \ud83d\udc2d {x.mouse_genotype} {x.diet_or_media} inflam\"\n",
    "            label = f\"[{x.name}] {x.source_samples} {x.diet_or_media} inflam\"\n",
    "        elif x.status_mouse_inflamed == 'not_Inflamed':\n",
    "            # label = f\"[{x.name}] {x.source_samples} \ud83d\udc2d {x.mouse_genotype} {x.diet_or_media} not_inf\"\n",
    "            label = f\"[{x.name}] {x.source_samples} {x.diet_or_media} not_inf\"\n",
    "        else:\n",
    "            raise ValueError(f\"sample type {x.status_mouse_inflamed} not understood\")\n",
    "    else:\n",
    "        raise ValueError(f\"sample type {x.sample_type} not understood\")\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "plt.rcParams[\"figure.dpi\"] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_order = [\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"H\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (\n",
    "    pd.read_table(\"meta/een-mgen/sample.tsv\")\n",
    "    .assign(\n",
    "        label=lambda x: x[\n",
    "            [\"collection_date_relative_een_end\", \"diet_or_media\", \"sample_id\"]\n",
    "        ].apply(tuple, axis=1)\n",
    "    )\n",
    "    .set_index(\"sample_id\")\n",
    "    .assign(full_label=lambda d: d.apply(label_een_experiment_sample, axis=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotu_counts = pd.read_table(\n",
    "    \"data/group/een/a.proc.zotu_counts.tsv\", index_col=\"#OTU ID\"\n",
    ").rename_axis(index=\"zotu\", columns=\"sample_id\")\n",
    "rotu_taxonomy = rotu_counts.taxonomy\n",
    "rotu_counts = rotu_counts.drop(columns=[\"taxonomy\"]).T\n",
    "rotu_rabund = rotu_counts.divide(rotu_counts.sum(1), axis=0)\n",
    "\n",
    "sample_rotu_bc_linkage = sp.cluster.hierarchy.linkage(\n",
    "    rotu_rabund, method=\"average\", metric=\"braycurtis\", optimal_ordering=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motu_depth = (pd.read_table(\n",
    "    \"data/group/een/r.proc.gene99_new-v22-agg75.spgc_specgene-ref-t25-p95.species_depth.tsv\",\n",
    "    names=['sample', \"species_id\", 'depth'], index_col=['sample', \"species_id\"],\n",
    "    )\n",
    "    .depth.unstack(fill_value=0)\n",
    "    .rename(columns=str, index=lambda x: \"CF_\" + str(int(x.split(\"_\")[1])))\n",
    "    .rename({'CF_15': 'CF_11', 'CF_11': 'CF_15'})  # Sample swap\n",
    ")\n",
    "motu_rabund = motu_depth.divide(motu_depth.sum(1), axis=0)\n",
    "\n",
    "motu_rabund"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "motu_depth = pd.read_table(\n",
    "    \"data/group/een/species/sp-102506/r.proc.gene99_new-v22-agg75.spgc_specgene-ref-t25-p95.species_depth.tsv\",\n",
    ").rename_axis(index=\"species_id\", columns=\"sample_id\")\n",
    "rotu_rabund = rotu_counts.divide(rotu_counts.sum(1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sotu_depth = []\n",
    "missing_files = []\n",
    "for species_id in motu_depth.columns:\n",
    "    path = f\"data/group/een/species/sp-{species_id}/r.proc.gtpro.sfacts-fit.comm.tsv\"\n",
    "    try:\n",
    "        d = (\n",
    "            pd.read_table(path, index_col=[\"sample\", \"strain\"])\n",
    "            .squeeze()\n",
    "            .unstack()\n",
    "            .rename(columns=str, index=lambda x: \"CF_\" + str(int(x.split(\"_\")[1])))\n",
    "            .rename({'CF_11': 'CF_15', 'CF_15': 'CF_11'})  # Sample swap.\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        missing_files.append(path)\n",
    "        d = pd.DataFrame([])\n",
    "    _keep_strains = idxwhere(d.sum() > 0.05)\n",
    "    assert d.index.isin(motu_depth.index).all()\n",
    "    d = d.reindex(index=motu_depth.index, columns=_keep_strains, fill_value=0)\n",
    "    d = d.assign(__other=lambda x: 1 - x.sum(1)).rename(columns={\"__other\": -1})\n",
    "    d[d < 0] = 0\n",
    "    d = d.divide(d.sum(1), axis=0)\n",
    "    d = d.multiply(motu_depth[species_id], axis=0)\n",
    "    d = d.rename(columns=lambda s: f\"{species_id}_{s}\")\n",
    "    sotu_depth.append(d)\n",
    "sotu_depth = pd.concat(sotu_depth, axis=1)\n",
    "sotu_rabund = sotu_depth.divide(sotu_depth.sum(1), axis=0)\n",
    "len(motu_depth.columns), len(missing_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_taxonomy_string(taxonomy_string):\n",
    "    values = taxonomy_string.split(\";\")\n",
    "    return pd.Series(values, index=[\"d__\", \"p__\", \"c__\", \"o__\", \"f__\", \"g__\", \"s__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motu_taxonomy_inpath = \"ref/uhgg_genomes_all_v2.tsv\"\n",
    "\n",
    "_motu_taxonomy = (\n",
    "    pd.read_table(motu_taxonomy_inpath)[lambda x: x.Genome == x.Species_rep]\n",
    "    .assign(species_id=lambda x: \"1\" + x.MGnify_accession.str.split(\"-\").str[2])\n",
    "    .set_index(\"species_id\")\n",
    ")\n",
    "\n",
    "# motu_lineage_string = _motu_taxonomy.Lineage\n",
    "\n",
    "motu_taxonomy = _motu_taxonomy.Lineage.apply(\n",
    "    parse_taxonomy_string\n",
    ")  # .assign(taxonomy_string=motu_lineage_string)\n",
    "motu_taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_test(d):\n",
    "    try:\n",
    "        res = sp.stats.wilcoxon(d[\"EEN\"], d[\"PostEEN\"])\n",
    "    except ValueError:\n",
    "        res = (np.nan, np.nan)\n",
    "    log2_ratio = np.log2(d[\"PostEEN\"] / d[\"EEN\"])\n",
    "    return pd.Series(\n",
    "        [log2_ratio.mean(), d[\"EEN\"].mean(), d[\"PostEEN\"].mean(), res[1]],\n",
    "        index=[\"log2_ratio\", \"mean_EEN\", \"mean_PostEEN\", \"pvalue\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motu_enrichment_results = (\n",
    "    motu_rabund.apply(lambda x: x + x.replace({0: np.inf}).min())\n",
    "    .join(sample[[\"subject_id\", \"diet_or_media\"]])\n",
    "    .groupby([\"subject_id\", \"diet_or_media\"])\n",
    "    .mean()\n",
    "    .stack()\n",
    "    .unstack(\"diet_or_media\")[[\"EEN\", \"PostEEN\"]]\n",
    "    .dropna()\n",
    "    .assign(log2_ratio=lambda x: np.log2(x[\"PostEEN\"] / x[\"EEN\"]))\n",
    "    .rename_axis(index=[\"subject_id\", \"motu_id\"])\n",
    "    .groupby(level=\"motu_id\")\n",
    "    .apply(enrichment_test)\n",
    ")\n",
    "motu_enrichment_results.sort_values(\"pvalue\", ascending=True).head(20)\n",
    "# fig, ax = plt.subplots()\n",
    "# print(d.log2_ratio.mean())\n",
    "# print(sp.stats.wilcoxon(d['PostEEN'], d['EEN']))\n",
    "# ax.hist(d.log2_ratio, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motu_mean_rabund = (motu_rabund.apply(lambda x: x + x.replace({0: np.inf}).min())\n",
    "    .join(sample[[\"subject_id\"]])\n",
    "    .groupby([\"subject_id\"]).mean()).mean(0)\n",
    "motu_mean_rabund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_classifier(sample_typeA, sample_typeB):\n",
    "    return \":\".join(sorted(set([sample_typeA, sample_typeB])))\n",
    "\n",
    "\n",
    "def construct_turnover_analysis_data(\n",
    "    dmat,\n",
    "    meta,\n",
    "    sample_type_var,\n",
    "    stratum_var=None,\n",
    "    time_var=None,\n",
    "):\n",
    "    var_list = []\n",
    "    for var in [sample_type_var, stratum_var, time_var]:\n",
    "        if var is not None:\n",
    "            var_list.append(var)\n",
    "    meta = meta.reindex(dmat.index)[var_list].dropna()\n",
    "    dmat = dmat.loc[meta.index, meta.index]\n",
    "    data = []\n",
    "    for (i, idxA), (j, idxB) in product(enumerate(meta.index), repeat=2):\n",
    "        pair_data = {\n",
    "            \"sampleA\": idxA,\n",
    "            \"sampleB\": idxB,\n",
    "            \"sample_typeA\": meta.loc[idxA, sample_type_var],\n",
    "            \"sample_typeB\": meta.loc[idxB, sample_type_var],\n",
    "            \"diss\": dmat.loc[idxA, idxB],\n",
    "        }\n",
    "        if stratum_var is not None:\n",
    "            pair_data.update(\n",
    "                {\n",
    "                    \"stratumA\": meta.loc[idxA, stratum_var],\n",
    "                    \"stratumB\": meta.loc[idxB, stratum_var],\n",
    "                }\n",
    "            )\n",
    "        if time_var is not None:\n",
    "            pair_data.update(\n",
    "                {\"timeA\": meta.loc[idxA, time_var], \"timeB\": meta.loc[idxB, time_var]}\n",
    "            )\n",
    "        data.append(pair_data)\n",
    "    data = pd.DataFrame(\n",
    "        data,\n",
    "    )\n",
    "    data = data.assign(\n",
    "        pair_type=lambda x: x.apply(\n",
    "            lambda y: pair_classifier(y.sample_typeA, y.sample_typeB), axis=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if time_var:\n",
    "        data = data.assign(time_delta=lambda x: np.abs(x.timeB - x.timeA))\n",
    "\n",
    "    data = data[lambda x: (x.stratumA == x.stratumB) & (x.sampleA < x.sampleB)]\n",
    "    if stratum_var:\n",
    "        data = data.assign(stratum=lambda x: x.stratumA)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_id = '101378'\n",
    "inpath = f\"data/group/een/species/sp-{species_id}/r.proc.gtpro.sfacts-fit.world.nc\"\n",
    "sf_fit = (\n",
    "    sf.data.World.load(\n",
    "        inpath\n",
    "    )\n",
    "    .rename_coords(sample=lambda s: \"CF_{}\".format(int(s.split(\"_\")[1])))\n",
    "    .rename_coords(sample={\"CF_11\": \"CF_15\", \"CF_15\": \"CF_11\"})\n",
    "    # .drop_low_abundance_strains(0.01)\n",
    "    # .rename_coords(strain=str)\n",
    ")\n",
    "\n",
    "comm = sf_fit.community.to_pandas()\n",
    "comm_bc_pdist = pd.DataFrame(squareform(pdist(comm, metric='braycurtis')), index=comm.index, columns=comm.index)\n",
    "\n",
    "turnover_data = construct_turnover_analysis_data(\n",
    "    comm_bc_pdist,\n",
    "    meta=sample[lambda x: x.diet_or_media.isin(['EEN', 'PostEEN'])],\n",
    "    sample_type_var=\"diet_or_media\",\n",
    "    stratum_var=\"subject_id\",\n",
    "    time_var=\"collection_date_relative_een_end\",\n",
    ")\n",
    "\n",
    "formula = \"diss ~ 0 + pair_type + cr(time_delta, 4) + C(stratum, Sum)\"\n",
    "fit = smf.ols(formula, data=turnover_data).fit()\n",
    "\n",
    "coef_list = []\n",
    "for coef in ['pair_type[EEN]', 'pair_type[PostEEN]', 'pair_type[EEN:PostEEN]']:\n",
    "    if coef in fit.params:\n",
    "        coef_list.append(fit.params[coef])\n",
    "    else:\n",
    "        coef_list.append(np.nan)        \n",
    "\n",
    "for pair_type, d1 in turnover_data.groupby('pair_type'):\n",
    "    plt.scatter('time_delta', 'diss', data=d1, label=pair_type)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_coef_results = []\n",
    "all_pairs = []\n",
    "\n",
    "for species_id in motu_enrichment_results.index:\n",
    "    inpath = f\"data/group/een/species/sp-{species_id}/r.proc.gtpro.sfacts-fit.world.nc\"\n",
    "    if not os.path.exists(inpath):\n",
    "        print(f\"No sfacts fit for {species_id}.\")\n",
    "        continue\n",
    "    sf_fit = (\n",
    "        sf.data.World.load(\n",
    "            inpath\n",
    "        )\n",
    "        .rename_coords(sample=lambda s: \"CF_{}\".format(int(s.split(\"_\")[1])))\n",
    "        .rename_coords(sample={\"CF_11\": \"CF_15\", \"CF_15\": \"CF_11\"})\n",
    "        # .drop_low_abundance_strains(0.01)\n",
    "        # .rename_coords(strain=str)\n",
    "    )\n",
    "    \n",
    "    comm = sf_fit.community.to_pandas()\n",
    "    comm_bc_pdist = pd.DataFrame(squareform(pdist(comm, metric='braycurtis')), index=comm.index, columns=comm.index)\n",
    "    \n",
    "    turnover_data = construct_turnover_analysis_data(\n",
    "        comm_bc_pdist,\n",
    "        meta=sample[lambda x: x.diet_or_media.isin(['EEN', 'PostEEN'])],\n",
    "        sample_type_var=\"diet_or_media\",\n",
    "        stratum_var=\"subject_id\",\n",
    "        time_var=\"collection_date_relative_een_end\",\n",
    "    )\n",
    "    all_pairs.append(turnover_data.assign(species_id=species_id))\n",
    "    if turnover_data.empty:\n",
    "        print(f\"No data for {species_id}.\")\n",
    "        continue\n",
    "    \n",
    "    formula = \"diss ~ 0 + pair_type + cr(time_delta, 4) + C(stratum, Sum)\"\n",
    "    try:\n",
    "        fit = smf.ols(formula, data=turnover_data).fit()\n",
    "    except ZeroDivisionError:\n",
    "        print(f\"OLS failed for {species_id}.\")\n",
    "        continue\n",
    "    except ValueError:\n",
    "        print(f\"OLS failed for {species_id}.\")\n",
    "        continue\n",
    "\n",
    "    coef_list = []\n",
    "    for coef in ['pair_type[EEN]', 'pair_type[PostEEN]', 'pair_type[EEN:PostEEN]']:\n",
    "        if coef in fit.params:\n",
    "            coef_list.append(fit.params[coef])\n",
    "        else:\n",
    "            coef_list.append(np.nan)        \n",
    "    lm_coef_results.append((species_id, *coef_list, turnover_data.shape[0], turnover_data.groupby('stratum').diss.mean().mean()))\n",
    "    \n",
    "    # for pair_type, d1 in turnover_data.groupby('pair_type'):\n",
    "    #     plt.scatter('time_delta', 'diss', data=d1, label=pair_type)\n",
    "    # plt.xscale('log')\n",
    "    # plt.legend()\n",
    "    # fit.summary()\n",
    "lm_coef_results = pd.DataFrame(lm_coef_results, columns=['motu_id', 'EEN', 'PostEEN', 'Transition', 'num_pairs', 'overall_mean_diss'])\n",
    "all_pairs = pd.concat(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = motu_enrichment_results.assign(overall_mean=motu_mean_rabund).dropna(subset=['overall_mean']).join(lm_coef_results.set_index('motu_id')).assign(\n",
    "    # relative_transition=lambda x: x['Transition'] - x['EEN'],\n",
    "    # signif_enrich=lambda w: w.pvalue < 0.05,\n",
    "    indicator_score=lambda x: x.log2_ratio * x.Transition**2 * x.mean_PostEEN * x.mean_EEN,\n",
    ")[lambda x: x.num_pairs > 20].dropna(subset=['log2_ratio', 'Transition'])\n",
    "\n",
    "plt.scatter('log2_ratio', 'overall_mean_diss', data=d.loc[['101493']], edgecolor='r', facecolor='none', s=200)\n",
    "plt.scatter('log2_ratio', 'overall_mean_diss', data=d, c='overall_mean', norm=mpl.colors.LogNorm())\n",
    "# plt.ylim(-0.5, 1.5)\n",
    "plt.colorbar(label='mean relative abundance')\n",
    "plt.xlabel('log2_fold_change')\n",
    "plt.ylabel('mean turn-over')\n",
    "# plt.yscale('symlog', linthresh=1e-2)\n",
    "# plt.xscale('log')\n",
    "\n",
    "print(sp.stats.spearmanr(d.log2_ratio, d.overall_mean_diss))\n",
    "d.sort_values('overall_mean_diss', ascending=False).head(50).join(motu_taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[lambda x: (x.log2_ratio > 0) & (x.log2_ratio < 2.5) & (x.overall_mean_diss < 0.2) & (x.overall_mean > 1e-2)].sort_values('overall_mean_diss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = motu_enrichment_results.join(lm_coef_results.set_index('motu_id')).assign(\n",
    "    # relative_transition=lambda x: x['Transition'] - x['EEN'],\n",
    "    # signif_enrich=lambda w: w.pvalue < 0.05,\n",
    ").dropna(subset=['log2_ratio', 'Transition'])\n",
    "\n",
    "plt.scatter('log2_ratio', 'Transition', data=d, c='mean_EEN', norm=mpl.colors.LogNorm())\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.colorbar()\n",
    "# plt.yscale('symlog', linthresh=1e-2)\n",
    "# plt.xscale('log')\n",
    "\n",
    "sp.stats.spearmanr(d.log2_ratio, d.Transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = motu_enrichment_results.join(lm_coef_results.set_index('motu_id')).assign(\n",
    "    relative_transition=lambda x: x['Transition'] - x['EEN'],\n",
    "    signif_enrich=lambda w: w.pvalue < 0.05,\n",
    ").dropna(subset=['relative_transition'])\n",
    "\n",
    "plt.scatter('mean_EEN', 'relative_transition', data=d, c='signif_enrich')\n",
    "plt.yscale('symlog', linthresh=1e-2)\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[lambda x: (x.mean_EEN > 1e-3) & (x.relative_transition > 1e-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[lambda x: (x.mean_EEN > 1e-3)].sort_values('relative_transition', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.spearmanr(d['log2_ratio'], d['relative_transition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sort_values('relative_transition', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "detect_thresh = 1e-4\n",
    "motu_detect_subject_sample_type = (motu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "sotu_detect_subject_sample_type = (sotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "rotu_detect_subject_sample_type = (rotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "\n",
    "\n",
    "subject_order = ['A', 'B', 'H']\n",
    "taxon_presence_data = dict(species=motu_detect_subject_sample_type)\n",
    "\n",
    "m_levels = len(taxon_presence_data)\n",
    "n_subjects = len(subject_order)\n",
    "\n",
    "fig, axs = plt.subplots(m_levels, n_subjects, figsize=(5 * n_subjects, 5 * m_levels), squeeze=False)\n",
    "\n",
    "shared_strains = {}\n",
    "for subject_id, ax_col in zip(subject_order, axs.T):\n",
    "    print(f\"Subject {subject_id}\")\n",
    "    print(f\"--------\")\n",
    "    for (taxon_level, tax_detect_subject_sample_type), ax in zip(taxon_presence_data.items(), ax_col):\n",
    "        print(taxon_level, \"num detected\", sep='\\t')\n",
    "        d = tax_detect_subject_sample_type.loc[subject_id].T\n",
    "        print(d['human'].sum(), f\"across all human samples\", sep='\\t')\n",
    "        print(d['Fermenter'].sum(), f\"across all fermenter samples\", sep='\\t')\n",
    "        print(d['mouse'].sum(), f\"across all mouse samples\", sep='\\t')\n",
    "        print((d['human'] & ~(d['Fermenter'] | d['mouse'])).sum(), f\"in humans and NOT mouse or fermenter\", sep='\\t')\n",
    "        print(((d['Fermenter'] | d['mouse']) & ~d['human']).sum(), f\"in mouse or fermenter and NOT human\", sep='\\t')\n",
    "        print((d['mouse'] & ~d['Fermenter']).sum(), f\"in mouse and NOT fermenter\", sep='\\t')\n",
    "        print((d['Fermenter'] & ~d['mouse']).sum(), f\"in fermenter and NOT mouse\", sep='\\t')\n",
    "        print()\n",
    "        venn3([set(idxwhere(d['human'])), set(idxwhere(d['Fermenter'])), set(idxwhere(d['mouse']))], ax=ax, set_labels=('h', 'f', 'm'))\n",
    "        ax.set_title(f\"Subject {subject_id}\")\n",
    "        shared_strains[subject_id] = (set(idxwhere(d['human'])) & set(idxwhere(d['Fermenter'])) & set(idxwhere(d['mouse'])))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_strains['A'] & shared_strains['B'] & shared_strains['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "detect_thresh = 1e-4\n",
    "motu_detect_subject_sample_type = (motu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "sotu_detect_subject_sample_type = (sotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "rotu_detect_subject_sample_type = (rotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "\n",
    "\n",
    "subject_order = ['A', 'B', 'H']\n",
    "taxon_presence_data = dict(strains=sotu_detect_subject_sample_type)\n",
    "\n",
    "m_levels = len(taxon_presence_data)\n",
    "n_subjects = len(subject_order)\n",
    "\n",
    "fig, axs = plt.subplots(m_levels, n_subjects, figsize=(5 * n_subjects, 5 * m_levels), squeeze=False)\n",
    "\n",
    "shared_strains = {}\n",
    "for subject_id, ax_col in zip(subject_order, axs.T):\n",
    "    print(f\"Subject {subject_id}\")\n",
    "    print(f\"--------\")\n",
    "    for (taxon_level, tax_detect_subject_sample_type), ax in zip(taxon_presence_data.items(), ax_col):\n",
    "        print(taxon_level, \"num detected\", sep='\\t')\n",
    "        d = tax_detect_subject_sample_type.loc[subject_id].T\n",
    "        print(d['human'].sum(), f\"across all human samples\", sep='\\t')\n",
    "        print(d['Fermenter'].sum(), f\"across all fermenter samples\", sep='\\t')\n",
    "        print(d['mouse'].sum(), f\"across all mouse samples\", sep='\\t')\n",
    "        print((d['human'] & ~(d['Fermenter'] | d['mouse'])).sum(), f\"in humans and NOT mouse or fermenter\", sep='\\t')\n",
    "        print(((d['Fermenter'] | d['mouse']) & ~d['human']).sum(), f\"in mouse or fermenter and NOT human\", sep='\\t')\n",
    "        print((d['mouse'] & ~d['Fermenter']).sum(), f\"in mouse and NOT fermenter\", sep='\\t')\n",
    "        print((d['Fermenter'] & ~d['mouse']).sum(), f\"in fermenter and NOT mouse\", sep='\\t')\n",
    "        print()\n",
    "        venn3([set(idxwhere(d['human'])), set(idxwhere(d['Fermenter'])), set(idxwhere(d['mouse']))], ax=ax, set_labels=('h', 'f', 'm'))\n",
    "        ax.set_title(f\"Subject {subject_id}\")\n",
    "        shared_strains[subject_id] = (set(idxwhere(d['human'])) & set(idxwhere(d['Fermenter'])) & set(idxwhere(d['mouse'])))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_strains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "detect_thresh = 1e-3\n",
    "motu_detect_subject_sample_type = (motu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "sotu_detect_subject_sample_type = (sotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "rotu_detect_subject_sample_type = (rotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "\n",
    "\n",
    "subject_order = ['A', 'B', 'H']\n",
    "taxon_presence_data = dict(zotus=rotu_detect_subject_sample_type, species=motu_detect_subject_sample_type, strains=sotu_detect_subject_sample_type)\n",
    "\n",
    "m_levels = len(taxon_presence_data)\n",
    "n_subjects = len(subject_order)\n",
    "\n",
    "fig, axs = plt.subplots(n_subjects, m_levels, figsize=(5 * m_levels, 5 * n_subjects))\n",
    "\n",
    "for subject_id, ax_row in zip(subject_order, axs):\n",
    "    print(f\"Subject {subject_id}\")\n",
    "    print(f\"--------\")\n",
    "    for (taxon_level, tax_detect_subject_sample_type), ax in zip(taxon_presence_data.items(), ax_row):\n",
    "        print(taxon_level, \"num detected\", sep='\\t')\n",
    "        d = tax_detect_subject_sample_type.loc[subject_id].T\n",
    "        print(d['human'].sum(), f\"across all human samples\", sep='\\t')\n",
    "        print(d['Fermenter'].sum(), f\"across all fermenter samples\", sep='\\t')\n",
    "        print(d['mouse'].sum(), f\"across all mouse samples\", sep='\\t')\n",
    "        print((d['human'] & ~(d['Fermenter'] | d['mouse'])).sum(), f\"in humans and NOT mouse or fermenter\", sep='\\t')\n",
    "        print(((d['Fermenter'] | d['mouse']) & ~d['human']).sum(), f\"in mouse or fermenter and NOT human\", sep='\\t')\n",
    "        print((d['mouse'] & ~d['Fermenter']).sum(), f\"in mouse and NOT fermenter\", sep='\\t')\n",
    "        print((d['Fermenter'] & ~d['mouse']).sum(), f\"in fermenter and NOT mouse\", sep='\\t')\n",
    "        print()\n",
    "        venn3([set(idxwhere(d['human'])), set(idxwhere(d['Fermenter'])), set(idxwhere(d['mouse']))], ax=ax, set_labels=('h', 'f', 'm'))\n",
    "        ax.set_title((taxon_level, subject_id))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "\n",
    "\n",
    "detect_thresh = 1e-4\n",
    "motu_detect_subject_sample_type = (motu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['sample_type', 'subject_id']).any()\n",
    "sotu_detect_subject_sample_type = (sotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['sample_type', 'subject_id']).any()\n",
    "rotu_detect_subject_sample_type = (rotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['sample_type', 'subject_id']).any()\n",
    "\n",
    "\n",
    "sample_type_order = ['human', 'Fermenter', 'mouse']\n",
    "taxon_presence_data = dict(zotus=rotu_detect_subject_sample_type, species=motu_detect_subject_sample_type, strains=sotu_detect_subject_sample_type)\n",
    "\n",
    "m_levels = len(taxon_presence_data)\n",
    "n_types = len(sample_type_order)\n",
    "\n",
    "fig, axs = plt.subplots(n_types, m_levels, figsize=(5 * m_levels, 5 * n_types))\n",
    "\n",
    "for sample_type, ax_row in zip(sample_type_order, axs):\n",
    "    for (taxon_level, tax_detect_subject_sample_type), ax in zip(taxon_presence_data.items(), ax_row):\n",
    "        d = tax_detect_subject_sample_type.loc[sample_type].T\n",
    "        venn3([set(idxwhere(d['A'])), set(idxwhere(d['B'])), set(idxwhere(d['H']))], ax=ax, set_labels=('A', 'B', 'H'))\n",
    "        ax.set_title((taxon_level, sample_type))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_thresh = 1e-4\n",
    "motu_detect_subject_sample_type = (motu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "sotu_detect_subject_sample_type = (sotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "rotu_detect_subject_sample_type = (rotu_rabund > detect_thresh).join(sample[['subject_id', 'sample_type']]).groupby(['subject_id', 'sample_type']).any()\n",
    "\n",
    "\n",
    "sample_type_order = ['human', 'Fermenter', 'mouse']\n",
    "taxon_presence_data = dict(zotus=rotu_detect_subject_sample_type, species=motu_detect_subject_sample_type, strains=sotu_detect_subject_sample_type)\n",
    "# taxon_presence_data = dict(species=motu_detect_subject_sample_type)\n",
    "\n",
    "\n",
    "m_levels = len(taxon_presence_data)\n",
    "n_types = len(sample_type_order)\n",
    "\n",
    "fig, axs = plt.subplots(n_types, m_levels, figsize=(5 * m_levels, 5 * n_types), squeeze=False)\n",
    "\n",
    "\n",
    "# taxon_level = 'species'\n",
    "# d0 = taxon_presence_data[taxon_level].loc[['A', 'B', 'H']]\n",
    "# shared_species_list = idxwhere(d.rename_axis(columns='taxon').unstack('subject_id').T.human.unstack().all(1))\n",
    "# d1 = d0[shared_species_list]\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "for (taxon_level, tax_detect_subject_sample_type), ax_col in zip(taxon_presence_data.items(), axs.T):\n",
    "    d0 = taxon_presence_data[taxon_level].loc[['A', 'B', 'H']]\n",
    "    human_shared_taxa_list = idxwhere(d0.rename_axis(columns='taxon').unstack('subject_id').T.human.unstack().all(1))\n",
    "    d1 = d0[human_shared_taxa_list]\n",
    "    for sample_type, ax in zip(sample_type_order, ax_col):\n",
    "        d2 = d1.xs(sample_type, level='sample_type').apply(lambda x: set(idxwhere(x)), axis=1).to_dict()\n",
    "        venn3([d2['A'], d2['B'], d2['H']], ax=ax, set_labels=['A', 'B', 'H'])\n",
    "        ax.set_title((sample_type, taxon_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs.groupby(['species_id', 'pair_type', 'stratum']).apply(lambda x: x.diss.mean()).groupby(['species_id', 'pair_type']).mean().unstack('pair_type').sort_values(['EEN:PostEEN'], ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolz2",
   "language": "python",
   "name": "toolz2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}