{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as _os\n",
    "\n",
    "_os.chdir(_os.environ[\"PROJECT_ROOT\"])\n",
    "_os.path.realpath(_os.path.curdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from itertools import chain, product\n",
    "from tempfile import mkstemp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import sfacts as sf\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import xarray as xr\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lib.plot\n",
    "from lib.dissimilarity import load_dmat_as_pickle\n",
    "from lib.pandas_util import align_indexes, aligned_index, idxwhere, invert_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.thisproject.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "plt.rcParams[\"figure.dpi\"] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = (\n",
    "    pd.read_table(\"meta/species_group.tsv\")[lambda x: x.species_group_id == \"hmp2\"]\n",
    "    .species_id.astype(str)\n",
    "    .unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_taxonomy_string(taxonomy_string):\n",
    "    values = taxonomy_string.split(\";\")\n",
    "    return pd.Series(values, index=[\"d__\", \"p__\", \"c__\", \"o__\", \"f__\", \"g__\", \"s__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_taxonomy_inpath = \"ref/uhgg_genomes_all_v2.tsv\"\n",
    "\n",
    "species_taxonomy = (\n",
    "    pd.read_table(species_taxonomy_inpath)[lambda x: x.Genome == x.Species_rep]\n",
    "    .assign(species_id=lambda x: \"1\" + x.MGnify_accession.str.split(\"-\").str[2])\n",
    "    .set_index(\"species_id\")\n",
    "    .Lineage.apply(parse_taxonomy_string)\n",
    ")\n",
    "species_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylum_palette = lib.plot.construct_ordered_palette(\n",
    "    sorted(species_taxonomy.p__.unique()),\n",
    "    cm=\"tab10\",\n",
    ")\n",
    "\n",
    "# assert len(set(phylum_palette.values())) == len((phylum_palette.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgen = pd.read_table(\"meta/hmp2/mgen.tsv\", index_col=\"library_id\")\n",
    "preparation = pd.read_table(\"meta/hmp2/preparation.tsv\", index_col=\"preparation_id\")\n",
    "stool = pd.read_table(\"meta/hmp2/stool.tsv\", index_col=\"stool_id\")\n",
    "visit = pd.read_table(\"meta/hmp2/visit.tsv\", index_col=\"visit_id\")\n",
    "subject = pd.read_table(\"meta/hmp2/subject.tsv\", index_col=\"subject_id\")\n",
    "\n",
    "meta_all = (\n",
    "    mgen.join(preparation.drop(columns=\"library_type\"), on=\"preparation_id\")\n",
    "    .join(stool, on=\"stool_id\")\n",
    "    .join(visit, on=\"visit_id\", rsuffix=\"_\")\n",
    "    .join(subject, on=\"subject_id\")\n",
    "    .assign(\n",
    "        new_name=lambda x: (\n",
    "            x[[\"subject_id\", \"week_number\"]]\n",
    "            .assign(library_id=x.index)\n",
    "            .assign(week_number=lambda x: x.week_number.fillna(999).astype(int))\n",
    "            .apply(lambda x: \"_\".join(x.astype(str)), axis=1)\n",
    "        )\n",
    "    )\n",
    "    # .reset_index()\n",
    "    # .set_index('new_name')\n",
    ")\n",
    "\n",
    "library_id_to_new_name = meta_all.new_name\n",
    "\n",
    "assert not any(meta_all.subject_id.isna())\n",
    "\n",
    "# TODO: Rename samples based on subject and visit number\n",
    "# TODO: Drop duplicate stools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_depth = []\n",
    "_missing_species = []\n",
    "\n",
    "for species in tqdm(species_list):\n",
    "    inpath = f\"data/group/hmp2/species/sp-{species}/r.proc.gene99_v20-v23-agg75.spgc_specgene-ref-filt-p95.species_depth.tsv\"\n",
    "    if not os.path.exists(inpath):\n",
    "        _missing_species.append(species)\n",
    "        continue\n",
    "    data = pd.read_table(inpath, names=[\"sample\", \"depth\"]).assign(species=species)\n",
    "    species_depth.append(data)\n",
    "species_depth = (\n",
    "    pd.concat(species_depth)\n",
    "    .set_index([\"sample\", \"species\"])\n",
    "    .depth.unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "print(\n",
    "    len(_missing_species),\n",
    "    \"out of\",\n",
    "    len(species_list),\n",
    "    \"species are missing.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_thresh = 0.2\n",
    "\n",
    "species_found = species_depth > depth_thresh\n",
    "species_prevalence = (\n",
    "    species_found.groupby(meta_all.subject_id).any().mean().sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "species_rabund_when_found = species_depth.divide(species_depth.sum(1), axis=0).where(\n",
    "    species_found, np.nan\n",
    ")\n",
    "species_mean_rabund_when_found = (\n",
    "    species_rabund_when_found.groupby(meta_all.subject_id)\n",
    "    .mean()\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "species_median_rabund_when_found = (\n",
    "    species_rabund_when_found.groupby(meta_all.subject_id)\n",
    "    .median()\n",
    "    .median()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "species_prevalence.to_frame(\"prevalence\").assign(\n",
    "    mean_rabund=species_mean_rabund_when_found,\n",
    "    median_rabund=species_median_rabund_when_found,\n",
    ").join(species_taxonomy).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strain Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_genome(x):\n",
    "    if (x.genome_type == \"Isolate\") & x.passes_filter:\n",
    "        return \"isolate\"\n",
    "    elif (x.genome_type == \"Isolate\") & ~x.passes_filter:\n",
    "        return \"isolate_fails_qc\"\n",
    "    elif (x.genome_type == \"MAG\") & x.passes_filter:\n",
    "        return \"mag\"\n",
    "    elif (x.genome_type == \"MAG\") & ~x.passes_filter:\n",
    "        return \"mag_fails_qc\"\n",
    "    elif (x.genome_type == \"SPGC\") & x.passes_filter:\n",
    "        return \"spgc\"\n",
    "    elif (\n",
    "        x.genome_type == \"SPGC\"\n",
    "    ) & x.passes_geno_positions:  # & x.passes_in_sample_list:  # NOTE: This last condition \"passes_in_sample_list\" is deprecated because we're now splitting out the different datasets.\n",
    "        return \"sfacts_only\"\n",
    "    elif (x.genome_type == \"SPGC\") & ~(\n",
    "        x.passes_geno_positions  # & x.passes_in_sample_list  # NOTE: This last condition \"passes_in_sample_list\" is deprecated because we're now splitting out the different datasets.\n",
    "    ):\n",
    "        return \"sfacts_fails_qc\"\n",
    "    else:\n",
    "        raise ValueError(\"Genome did not match classification criteria:\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_stats = []\n",
    "missing = []\n",
    "\n",
    "for species in tqdm(species_list):\n",
    "    inpath = f\"data/group/hmp2/species/sp-{species}/r.proc.gtpro.sfacts-fit.gene99_v20-v23-agg75.spgc-fit.strain_meta_spgc_and_ref.tsv\"\n",
    "    if not os.path.exists(inpath):\n",
    "        missing.append(inpath)\n",
    "        continue\n",
    "    data = pd.read_table(inpath).assign(species=species, inpath=inpath)\n",
    "    filt_stats.append(data)\n",
    "print(\n",
    "    len(missing),\n",
    "    \"out of\",\n",
    "    len(species_list),\n",
    "    \"species are missing stats.\",\n",
    ")\n",
    "\n",
    "filt_stats = pd.concat(filt_stats).assign(\n",
    "    genome_class=lambda x: x.apply(classify_genome, axis=1),\n",
    "    species_strain=lambda x: x.species + \"_\" + x.genome_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect strain fractions:\n",
    "\n",
    "strain_depth = []\n",
    "strain_to_species = []\n",
    "missing_species = []\n",
    "for species in tqdm(species_list):\n",
    "    inpath = f\"data/group/hmp2/species/sp-{species}/r.proc.gtpro.sfacts-fit.comm.tsv\"\n",
    "    if not os.path.exists(inpath):\n",
    "        missing_species.append(species)\n",
    "        d = pd.DataFrame([])\n",
    "    else:\n",
    "        d = pd.read_table(inpath, index_col=[\"sample\", \"strain\"]).squeeze().unstack()\n",
    "\n",
    "    if species in species_depth.columns:\n",
    "        _species_depth = species_depth[species]\n",
    "    else:\n",
    "        _species_depth = 0\n",
    "\n",
    "    _keep_strains = idxwhere(d.sum() > 0.05)\n",
    "    assert d.index.isin(species_depth.index).all()\n",
    "    d = d.reindex(index=species_depth.index, columns=_keep_strains, fill_value=0)\n",
    "    d = d.assign(__other=lambda x: 1 - x.sum(1)).rename(columns={\"__other\": -1})\n",
    "    d[d < 0] = 0\n",
    "    d = d.divide(d.sum(1), axis=0)\n",
    "    d = d.multiply(_species_depth, axis=0)\n",
    "    d = d.rename(columns=lambda s: f\"{species}_{s}\")\n",
    "    strain_depth.append(d)\n",
    "    strain_to_species.append(pd.Series(species, index=d.columns))\n",
    "print(\n",
    "    len(missing_species),\n",
    "    \"out of\",\n",
    "    len(species_list),\n",
    "    \"species are missing stats.\",\n",
    ")\n",
    "\n",
    "strain_depth = pd.concat(strain_depth, axis=1)\n",
    "strain_rabund = strain_depth.divide(strain_depth.sum(1), axis=0)\n",
    "strain_rabund[\"-1\"] = 1 - strain_rabund.sum(1)\n",
    "strain_to_species = pd.concat(strain_to_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = meta_all.loc[strain_depth.index].subject_id.value_counts()\n",
    "d.agg([\"count\"]), d.quantile([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared strains between\n",
    "strain_detection_threshold = 0.1\n",
    "\n",
    "# Select species_strain (concatenated name) where it's StrainFacts inferred and passes the geno_positions filter.\n",
    "species_strain_list = filt_stats[\n",
    "    lambda x: x.passes_geno_positions & x.genome_type.isin([\"SPGC\"])\n",
    "].species_strain\n",
    "\n",
    "# other_strain_list = idxwhere(strain_depth.columns.to_series().str.endswith(\"-1\"))\n",
    "strain_present = strain_depth > strain_detection_threshold\n",
    "low_strain_samples = idxwhere(strain_present.sum(1) <= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3A/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_by_sample = strain_present[species_strain_list]\n",
    "strain_by_subject = (\n",
    "    strain_by_sample.groupby(meta_all.subject_id).any().loc[:, strain_by_sample.any()]\n",
    ")  # Exclude undetected strains.\n",
    "\n",
    "total_strains_per_subject = strain_by_subject.sum(1)\n",
    "total_subjects_per_strain = strain_by_subject.sum(0)\n",
    "total_subjects_per_common_strain = total_subjects_per_strain[strain_by_sample.sum() > 1]\n",
    "\n",
    "print(\"total_strains_per_sample\", strain_by_sample.sum(1).quantile([0.25, 0.5, 0.75]))\n",
    "print(\n",
    "    \"total_strains_per_subject\", total_strains_per_subject.quantile([0.25, 0.5, 0.75])\n",
    ")\n",
    "print(\"num common strains\", len(total_subjects_per_common_strain))\n",
    "print(\n",
    "    \"total_subjects_per_strain\",\n",
    "    (total_subjects_per_strain == 0).mean(),\n",
    "    (total_subjects_per_strain == 1).mean(),\n",
    "    (total_subjects_per_strain >= 3).mean(),\n",
    ")\n",
    "print(\n",
    "    \"total_subjects_per_common_strain\",\n",
    "    (total_subjects_per_common_strain == 0).mean(),\n",
    "    (total_subjects_per_common_strain <= 1).mean(),\n",
    "    (total_subjects_per_common_strain <= 2).mean(),\n",
    ")\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(3.0, 2.5), gridspec_kw=dict(hspace=0.85))\n",
    "ax = axs[0]\n",
    "bins = np.linspace(0, total_strains_per_subject.max(), num=20)\n",
    "ax.hist(total_strains_per_subject, bins=bins, density=False, color=\"k\", histtype=\"step\")\n",
    "ax.hist(total_strains_per_subject, bins=bins, density=False, color=\"k\", alpha=0.5)\n",
    "# ax.set_ylabel('Subjects')\n",
    "ax.set_xlabel(\"Strains\")\n",
    "\n",
    "# fig.savefig('fig/spgc_strains_per_subject.svg', bbox_inches='tight')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(2, 1))\n",
    "ax = axs[1]\n",
    "bins = np.arange(1, 15)\n",
    "ax.hist(\n",
    "    total_subjects_per_common_strain,\n",
    "    bins=bins,\n",
    "    density=False,\n",
    "    color=\"k\",\n",
    "    histtype=\"step\",\n",
    "    align=\"left\",\n",
    ")\n",
    "ax.hist(\n",
    "    total_subjects_per_common_strain,\n",
    "    bins=bins,\n",
    "    density=False,\n",
    "    color=\"k\",\n",
    "    alpha=0.5,\n",
    "    align=\"left\",\n",
    ")\n",
    "ax.set_xticks(np.arange(1, 14, step=2))\n",
    "# ax.set_ylabel('Strains')\n",
    "# axs[1].set_yticks([0])\n",
    "ax.set_xlabel(\"Subjects\")\n",
    "fig.savefig('fig/fig3ab_strain_subject.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of strains found in at least 2 samples:\n",
    "d0 = strain_by_subject.loc[:, (strain_by_sample.sum() >= 2)]\n",
    "d1 = d0.sum()\n",
    "\n",
    "(d1 == 0).mean(), (d1 == 1).mean(), (d1 >= 3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, x = align_indexes(\n",
    "    meta_all, strain_present[species_strain_list].drop(low_strain_samples)\n",
    ")\n",
    "\n",
    "shared_strains = pdist(x, metric=lambda x, y: (x & y).sum())\n",
    "diff_subj = pdist(m[[\"subject_id\"]], lambda x, y: (x != y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    2,\n",
    "    figsize=(3, 1.5),\n",
    "    gridspec_kw=dict(height_ratios=(0.4, 0.6), hspace=0.1),\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "\n",
    "# bins = list(np.arange(0, 10)) + list(np.linspace(10, shared_strains.max(), num=10))\n",
    "bins = np.arange(0, shared_strains.max())\n",
    "for ax in axs:\n",
    "    for _diff_subj, label, c in zip(\n",
    "        [0, 1], [\"Same Subject\", \"Different Subject\"], [\"tab:purple\", \"tab:red\"]\n",
    "    ):\n",
    "        ax.hist(\n",
    "            shared_strains[diff_subj == _diff_subj],\n",
    "            alpha=0.5,\n",
    "            color=c,\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            label=label,\n",
    "        )\n",
    "        ax.hist(\n",
    "            shared_strains[diff_subj == _diff_subj],\n",
    "            histtype=\"step\",\n",
    "            color=c,\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            label=\"__nolegend__\",\n",
    "        )\n",
    "        print(\n",
    "            f\"diff_subject: {_diff_subj}, mean:\",\n",
    "            shared_strains[diff_subj == _diff_subj].mean(),\n",
    "        )\n",
    "axs[0].set_ylim(bottom=0.5, top=0.62)\n",
    "axs[1].set_ylim(top=0.31)\n",
    "axs[1].set_xlabel(\"Shared Strains per Sample Pair\")\n",
    "axs[1].set_ylabel(\"           Pairs (fraction)\")\n",
    "axs[0].legend(fontsize=8)\n",
    "\n",
    "axs[1].spines[\"top\"].set_visible(False)\n",
    "axs[0].spines[\"bottom\"].set_visible(False)\n",
    "axs[0].set_xticks([])\n",
    "\n",
    "axs[0].set_yticks([0.25, 0.5])\n",
    "axs[0].set_ylim(0.05, 0.65)\n",
    "axs[1].set_ylim(0.0, 0.05)\n",
    "axs[1].set_yticks([0.0, 0.02, 0.04])\n",
    "\n",
    "aspect = \"auto\"\n",
    "axs[0].set_aspect(aspect)\n",
    "axs[1].set_aspect(aspect)\n",
    "\n",
    "fig.savefig('fig/fig3c_strain_sharing.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.stats\n",
    "\n",
    "lib.stats.mannwhitneyu(\n",
    "    \"x\", \"y\", data=pd.DataFrame(dict(x=diff_subj.astype(bool), y=shared_strains))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stats.mannwhitneyu(\n",
    "    shared_strains[diff_subj.astype(bool)], shared_strains[~diff_subj.astype(bool)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared strains between\n",
    "other_strain_list = idxwhere(strain_depth.columns.to_series().str.endswith(\"-1\"))\n",
    "x = (strain_depth.groupby(meta_all.subject_id).max() > 0.1).drop(\n",
    "    columns=other_strain_list\n",
    ")\n",
    "\n",
    "shared_strains_between_subjects = pdist(x, metric=lambda x, y: (x & y).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(shared_strains[diff_subj.astype(bool)]).quantile(\n",
    "    [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toolz5",
   "language": "python",
   "name": "toolz5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}